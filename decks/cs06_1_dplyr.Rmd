---
title: Web Scraping - dplyr
output: 
  revealjs::revealjs_presentation:
    theme: night
    highlight: espresso
    center: true
    transition: none
    css: styles.css
    fig_caption: true
    reveal_options:
      progress: true
      slideNumber: true
    html_document:
    self_contained: no
---
## Preparation{.smaller}
- Read [W3 Schools HTML Intro](https://www.w3schools.com/html/html_intro.asp) for a brief introduction to HTML, the "markup language" for web pages
- Read [W3 Schools XML Intro](https://www.w3schools.com/xml/xml_whatis.asp) for a brief introuction to XML
- Read [W3 Schools XML DOM Intro](https://www.w3schools.com/xml/dom_intro.asp) and [XML DOM Nodes](https://www.w3schools.com/xml/dom_nodes.asp) to understand how to describe which node we want the data from
- Read [W3 Schools XPath Intro](https://www.w3schools.com/xml/xpath_intro.asp), [XPath Nodes](https://www.w3schools.com/xml/xpath_nodes.asp), and [XPath Syntax](https://www.w3schools.com/xml/xpath_syntax.asp)

---

## Hypertext Markup Language
###### citation: Colin Rundell STA 523

Most of the data on the web is still largely available as HTML. Some pages are moving to JavaScript





```html
<html>
  <head>
    <title>This is a title</title>
  </head>
  <body>
    <p align="center">Hello world!</p>
    <br/>
    <div class="name" id="first">John</div>
    <div class="name" id="last">Doe</div>
    <div class="contact">
      <div class="home">555-555-1234</div>
      <div class="home">555-555-2345</div>
      <div class="work">555-555-9999</div>
      <div class="fax">555-555-8888</div>
    </div>
  </body>
</html>
```

---

## SelectorGadget
###### citation: Colin Rundell STA 523

This is a javascript based tool that helps you interactively build an appropriate CSS selector for the content you are interested in.


<center>
<a href='http://selectorgadget.com/'>http://selectorgadget.com/</a>
</center>


---

## Scraping CyberCoders.com
## {.smaller}
Like many of the job posting sites, [cybercoders.com](https://www.cybercoders.com) allows us to specify a search query to find the jobs of interest. Let's investigate one of the job postings under "data scientist" and generalize our code later on. 
```{r libs,message=FALSE}
library(rvest)
library(XML)
library(RCurl)
library(dplyr)

url = "https://www.cybercoders.com/data-scientist-job-431683"

# parse the URL
page = read_html(url)
```

---

## Words in Job Description{.smaller}
There are a few paragraphs of job description. We can inspect this web page in the browser, and move the cursor over part of the page and see the corresponding HTML elements(s). We found that those descriptions are all inside DIV nodes with a class attribute "section-data section-data-title;" we can extract them using XPath.


```{r Vac, echo=FALSE}
knitr::include_graphics(".\\figures\\words.png")
```

---

```{r htmlnodes, message=FALSE}

words = page %>%
  html_nodes('.section-data-title') %>% # get the corresponding html nodes
  html_text() # extract text under those html nodes

```

---

## Location and Salary Information{.smaller}
The posts on cybercoders.com contain the location and salary on top of each page. For the location information, we are looking for a SPAN node under a DIV class with class attribute "location". We can see two qualified notes, but the first one is empty. We want the second note. As for the salary, we want a DIV node with class attribute "wage".
```{r locsal, message=FALSE}
# location information
job_location = page %>%
  html_node('.pin+ span') %>%
  html_text()
# salary information
job_salary = page %>%
  html_node('.money+ span') %>%
  html_text

c(job_location,job_salary)
```


---

## Skill List{.smaller}
There are a list of preferred skills at the bottom of the post. The content of each skill is within a LI (list item) element with a class attribute "skill-item". The actual text is within a SPAN element with skill-name as the class attribute. 
```{r skill, message=FALSE}
skill_list_pipe = page %>%
  html_nodes('.skill-name') %>%
  html_text()
```
---

## Date Posted{.smaller}
The web page also shows the date of posting on top. We can get this information from the <span> element under DIV node with a class attribute "mobile-hide posted posted-text".
```{r date, message=FALSE}
date_posted_pipe = page %>%
  html_node('.posted-text span') %>%
  html_text()
```

---

## Combine these steps{.smaller}
```{r combo,message=FALSE}
## Helper function
extract_text = function(page, tag){
  res = page %>% 
    html_nodes(tag) %>%
    html_text()
  
  return(res)
}

# Combining all steps, each with its own function
cy.getFreeFormWords <- function(page){
  words = extract_text(page, '.section-data-title')
  return(words)
}

cy.getDatePosted <- function(page){
  temp = extract_text(page, '.posted-text span')
  
  if (length(temp) == 0){
    mydate = format(Sys.Date(), "%Y/%m/%d")
    }
  else{
    mydate = strsplit(temp," ")[[1]][2]
  }
  
  return(mydate)
}

cy.getSkillList <- function(page){
  skillList = extract_text(page, '.skill-name')
  return(skillList)
}

cy.getLocationSalary <- function(page){
  Location = extract_text(page, '.pin+ span')
  Salary = extract_text(page, '.money+ span')
  
  return(c(Location,Salary))
}

cy.readPost <- function(url){
  page = read_html(url)
  ans = list(Words = cy.getFreeFormWords(page),
             datePosted = cy.getDatePosted(page),
             Skills = cy.getSkillList(page),
             Location = cy.getLocationSalary(page)[1],
             Salary = cy.getLocationSalary(page)[2]
             )
  ans
}

```

---

##{.smaller}
### Finding the Links to Job Postings in the Search Results{.smaller}
```{r links, message=FALSE, cache=TRUE}
# HTML when we type "Data Scientist" into the query textfield
u = "https://www.cybercoders.com/search/?searchterms=Data+Scientist&searchlocation=&newsearch=true&originalsearch=true&sorttype="
# get named parameters and their values
p = getFormParams(u)
p
# submit the query with getForm(), allowing us to vary the search query
txt = getForm("https://www.cybercoders.com/search/",
              searchterms = "Data Scientist",
              searchlocation = "",  newsearch = "true",
              originalsearch = "true", sorttype = "")
page2 = read_html(txt)
```

---

##{.smaller}
For the links to each job posting, we are looking for the *href* attribute value in the *a* element within the DIV with a class attribute with a value "job-title". Those links looks something like "/data-scientist-job-426052". This is a relative URL, relative to the base URL of our query, i.e.,  [http://www.cybercoders.com/search/](http://www.cybercoders.com/search/). We need to merge these links with this URL to get the full URL of the posts we want to scrape.
```{r links2, message=FALSE, cache=TRUE}
# get the links on the search page
job_lists = page2 %>%
  html_nodes('.job-title a') %>%
  html_attr('href')

# merge with the base URL
links = getRelativeURL(job_lists, "http://www.cybercoders.com/search/")
```

---

##{.smaller}

Then we can write a function to get the post links, and combine it with cy.readPost to have a function which can read all the posts on a search page.
```{r links3, message=FALSE, cache=TRUE}
cy.getPostLinks <- function(page, baseURL = "http://www.cybercoders.com/search/"){
  if(is.character(page)) {
    page = read_html(page)
  }
  
  links = page %>%
    html_nodes('.job-title a') %>%
    html_attr('href')
    
  getRelativeURL(links, baseURL)
}

cy.readPagePosts <- function(page, links = cy.getPostLinks(page, baseURL),
                             baseURL = "http://www.cybercoders.com/search/")
{
  if(is.character(page)) {
    page = read_html(page)
    }
  
  lapply(links, cy.readPost)
}

# Try out
posts = cy.readPagePosts(u)
head(sapply(posts, `[[`, "Location"))
```

---

##{.smaller}
### Finding the Next Page of Job Post Search Results{.smaller}
In order to find the link to next page, we look for an *a* node with a *rel* attribute containing the word "next". There are actually two of these, but they have identical *href* attributes, which is what we want.
```{r links4, message=FALSE, cache=TRUE}
cy.getNextPageLink <- function(u, baseURL, page = read_html(u)){
  if (is.na(baseURL)) {
    baseURL = "http://www.cybercoders.com/search/"
  }
  
  link = page %>%
    html_node('.next') %>%
    html_attr('href')
  
  # return an empty character vector if there is no next page link
  if (length(link)==0) {
    return(character())
    }
  
  # combine the relative URL with our base URL
  return(paste(baseURL,substring(link,3),sep = ""))
}

# test it
tmp = cy.getNextPageLink(u, baseURL = "http://www.cybercoders.com/search/")
tmp
```

---

## Putting It All Together {.smaller}
```{r final,message=FALSE, cache=TRUE}
cyberCoders <- function(query){
  txt = getForm("https://www.cybercoders.com/search/", 
                searchterms = query, 
                searchlocation = "", 
                newsearch = "true", 
                originalsearch = "true", 
                sorttype = "")
  
  page = read_html(txt)
  posts = list()
  
  Page_num = 1 # OPTIONAL
  tf = TRUE 
  while(tf){
    posts = c(posts, cy.readPagePosts(page))
    nextPage = cy.getNextPageLink(baseURL = NA, page = page)
    
    if (length(nextPage) == 0) {
      break
      }
    
    # nextPage = getURLContent(nextPage)
    page = read_html(nextPage)
    Page_num = Page_num + 1
    
    if(Page_num >2){# optional to limit number of pages to scrape
      tf = FALSE
    }
  }
  invisible(posts)
}

# Test it
dataSciPosts = cyberCoders("Data Scientist")
tt = sort(table(unlist(lapply(dataSciPosts,`[[`,"Skills"))),decreasing = TRUE)
tt[tt >= 2]

```

