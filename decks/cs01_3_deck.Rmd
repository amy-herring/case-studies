---
title: Robust Regression
output: 
  revealjs::revealjs_presentation:
    theme: night
    highlight: espresso
    center: true
    transition: none
    css: styles.css
    fig_caption: true
    reveal_options:
      progress: true
      slideNumber: true
      
  
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=6, fig.height=5)
```



## Birth weight

Birth weight is a commonly-used indicator of a newborn infant's health status. Because this indicator is easily obtained, it is a critical component of population health reporting by the World Health Organization and governments around the globe. The distribution of birthweight has often been described as approximately normal, though the left tail in particular is inflated relative to that from a Gaussian distribution.


## Dealing with heavy-tailed distributions
Birth weight has heavy tails, or more observations in the tail than a typical normal distribution.

Fitting a linear regression model assumes the data follow an approximate normal distribution. Instead, we may want to consider a parametric family that has heavier tails and therefore better accommodates outliers in the data.

Let's first start with linear regression.


## Read data into R
We will evaluate whether there is a relationship between the response, birth weight, and the predictors gestational age (measured in weeks) and sex.

* Check for NAs in data - appear as 9, 99, 9999 for our variables of interest (birth weight, gestational age, and sex)
* For now, remove the rows with missing values
* Focus on most recent year with data, 2016
```{r readdata}
#Read in birth data
o_data <- read.csv("~/Documents/TEACHING/vitalstats/Yr1116Birth.csv", 
  na.strings=c("9","99", "9999"))
#SEX=1 male, 2 female; male=1 male, 0 female
o_data$male=2-o_data$SEX #binary gender for interpretation

birth_data <- na.omit(o_data)
birth_data_2016=birth_data[which(birth_data$YOB==2016),]
```

## Is birth weight normally distributed?
* Compare histogram of birth weight to a normal distribution parameterized with a mean and variance that is set based on birth weight variable, `BWTG`.
* Do the distributions align?
  
```{r histogram, out.width = '50%', echo=FALSE }
par(mfrow=c(1,2))
hist(birth_data$BWTG, xlab="Birthweight (g)", main="Birthweight Histogram", prob=TRUE, breaks=50)
curve(dnorm(x, mean=mean(birth_data$BWTG), sd=sd(birth_data$BWTG)),
      col="darkblue", lwd=2, add=TRUE, yaxt="n")
hist(birth_data$BWTG, xlab="Birthweight (g)", main="Birth weight Histogram", prob=TRUE, xlim=c(0, 4000), breaks=30)
curve(dnorm(x, mean=mean(birth_data$BWTG), sd=sd(birth_data$BWTG)),
      col="darkblue", lwd=2, add=TRUE, yaxt="n")
```

## Review of linear regression
Let $y_i$ denote the observed response, 'BWTG', for observation $i$, and $x_i$ the observed covariates, 'GEST' and 'male'. Then we aim to fit the model:

$$ y_i = \beta_0 + \beta_1 \text{GEST}_i + \beta_2 \text{male}_i + \epsilon_i $$

When we fit the line, we estimate the coefficients $\mathbf{\widehat{\beta}}= \{\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\beta}_2\}$, where $\widehat{\beta} = (X^\prime X)^{-1} X^\prime Y$.

$$ \widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 \text{GEST}_i + \widehat{\beta}_2 \text{male}_i  $$
We denote residuals, $e_i = y_i - \widehat{y}_i$ where $\widehat{y}_i$ is the estimated predicted value for observation $i$.

## Model Assumptions
To make inference on the model parameters, $\beta$, we check four assumptions for the linear regression model:

1. The mean of the response, $E(y_i) = \beta_0 + \beta_1 \text{GEST}_i + \beta_2 \text{male}_i$ is **linear function** of the $x_i$ at each value of the predictors $x_i$.
2. The errors, $\epsilon_i$, are **independent**.
3. The errors, $\epsilon_i$, at each value of the predictor, $x_i$, are **normally distributed**.
4. The errors, $\epsilon_i$, at each value of the predictor, $x_i$, have **equal variances**.

Or in summary, we assume the errors, $\epsilon_i$, are independent, normally distributed with zero mean and constant variance:
$$\epsilon_i \overset{\text{iid}}{\sim} \text{Normal}(0, \sigma^2).$$


## Fit regression to data
Let's fit a linear model using the function `lm`. 
```{r firstmodel}
model1 = lm(BWTG~GEST+male, data=birth_data)
summary(model1)
```

## Look at residuals
What do the residuals tell us about model fit?
```{r residuals, out.width= '50%' }
par(mfrow=c(2,2))
plot(model1)
```


## Diagnosing the problem
Diagnostic plots of the model are essential for checking model fit. 

Heavy-tailed data leads to large residuals and large deviations on the QQplot

A few tools for heavy-tailed data:

* Deletion of outliers - often done in practice, but throwing away data is generally a very bad idea
* Transformations (e.g. sqrt(BTWG)) - transforming outcome may fix the skewness but can make interpretting the model more difficult
* Robust regression - use a more flexible regression model to accommodate the data. While there are many methods for robust regression, they share a general strategy of giving lower weight to observations that have large influence on the regression model fit.

## What do we mean by robust?

- We define *robustness* as a lack of sensitivity to small deviations from the model assumptions
- We have particular interest in distributional robustness and the impact of outliers or non-normal distributions on our estimates
- Thus we will call a model robust if it has the following properties
    - Reasonably efficient 
    - Small deviations from the model assumptions will not substantially impair model performance
    - Somewhat larger deviations will not completely invalidate model
    
## Robustness and linear regression

- Linear regression/ordinary least squares is not robust to outliers. In a small- to moderate-sized data set, one observation can have significant impact on estimates.
- Efficiency of least squares is hindered by heavy-tailed distributions
- Diagnostics can be used to detect outliers and influential points, but what do we do once we have identified them?  Is it appropriate to remove them from the data if they represent valid observations?  Often extreme observations (e.g., long-term survivors) are the most interesting in a data set.

<!---
## Bayesian robust regression in R
Consider the Bayesian approach for robust regression using package `brms`.

* We specify the model using the family, Student-t, to describe the response distribution
    + Student's t distribution has heavier tails compared to the normal distribution. 
    + The heaviness of the tails is specified with the degrees of freedom parameter. The larger the degrees of freedom, the closer to normal the distribution becomes.
```{r tregbayes, eval=FALSE}
install.packages("brms", dependencies=TRUE)
require(brms)
fit1 <- brm(formula = BWTG ~ GEST+male ,
            data = birth_data, family = student(), cores = 2)
```

-->


## M-estimation
 M-estimation simply solves estimating equations (similar to likelihood score equations) to obtain weighted estimators of the form $\widehat{\beta}^{(s)} = (X^\prime W^{(s-1)} X)^{-1} X^\prime W^{(s-1)} Y$, where $s$ is step $s$ in an interative algorithm, and $W$ is a diagonal weight matrix. An interative algorithm is needed because the weights generally depend on the residuals, and the residuals depend on the weights. In the special case in which $W$ is the identity matrix, we obtain maximum likelihood estimates (ordinary least squares); in fact, the "M" in M-estimation represents "maximum likelihood-type."
 
## M-estimation

The weight matrix $W$ can be defined in a variety of ways; a popular choice is the use of Huber weights, in which observations with small residuals are given weights of 1, and observations with larger residuals are downweighted:

$\begin{aligned}
w(\epsilon)= &1 ~~~~ \text{for} ~~ |\epsilon| \leq k \\ w(\epsilon)=&\frac{k}{|\epsilon|} ~ \text{for} ~~ |\epsilon|>k.
\end{aligned}$

## M-estimation

The value for $k$ is called a *tuning constant*; smaller values produce more resistance to outliers at the cost of reduced efficiency if errors are in fact normally distributed. The value of $k$ is often tied to $\sigma$; for example, [Fox and Weisberg](http://users.stat.umn.edu/~sandy/courses/8053/handouts/robust.pdf) note that $k=1.345\sigma$ provides around 95\% efficiency when errors are normal but still provides some protection against overly influential outliers. Because the estimate of $\sigma$ itself can be highly sensitive to outliers, often a robust estimate is used instead (e.g., $\frac{MAD}{0.6745}$), where $MAD=\text{median}(|y_i-\text{median}(y)|)$.

## M-estimation in R

We will explore frequentist robust regression via M-estimation using the function `rlm` in the `MASS` package. As a default, this package carries out estimation via M-estimation with Huber weights.

```{r freqrobust}
library(MASS)
fit2 <- rlm(BWTG ~ GEST+male, data = birth_data)
summary(fit2)
```
How do these results compare to those of ordinary least squares?

## M-estimation in R

This robust regression technique works by reweighting the observations with large residuals. Let's look at the observations with the largest and smallest weights. What do you notice?

```{r freqrobust2}
fit2_weights = data.frame(bwt = birth_data$BWTG, gest = birth_data$GEST, 
  resid=fit2$resid, weight=fit2$w)
fit2_weights[order(fit2$w)[c(1:5, (length(fit2$w)-5):length(fit2$w))],]
```
## Class Exercises:
* Evaluate the observations with largest residuals/weights and discuss possible reasons for the large residual
* Determine whether a linear function in gestational age is adequate
* Continue model-fitting with `rlm` exploring the remaining covariates
* Create a test set of observations and measure how well we can predict those values
    + Use mean squared error to measure prediction accuracy
* What predictors of birth weight could be modified by a woman during her pregnancy?
