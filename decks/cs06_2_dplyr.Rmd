---
title: Web Scraping Generalizations - dplyr
output: 
  revealjs::revealjs_presentation:
    theme: night
    highlight: espresso
    center: true
    transition: none
    css: styles.css
    fig_caption: true
    reveal_options:
      progress: true
      slideNumber: true
    html_document:
    self_contained: no
---
## A Reusable Generic Framework for Arbitrary Sites
##{.smaller}
There are 4 components to get the information from all of the job posts for a given query:

1. A mechanism to submit the search query and get the first page of results.
2. A means to extract the links to the individual job posts from a page of results.
3. A function to read the contents of an individual job post.
4. A way to find the next page of results, relative to the current page.

##{.smaller} 
General Framework

```{r,message=FALSE}
library(rvest)
library(XML)
library(RCurl)

# Given a search query, get the pages listing the jobs.
# we loop over these pages and harvest the 
# individual jobs in each.

searchJobs <- function(firstPage, getNextPage_f, getJobDescriptionLinks_f, getJobDescription_f,
           max = NA, curl = getCurlHandle(followlocation = TRUE)){
  
    curPage = firstPage
    jobs = list()
    
    pageNum = 1
    
    while(is.na(max) || length(jobs) < max) {
      listings_page = read_html(curPage)
      
      # extract the links to the individual job posts
      links = getJobDescriptionLinks_f(listings_page)
      
      # try(): if getJobDescription_f() raises an error for any reason for a particular post, we will catch the error here and continue on to the next post
      posts = structure(lapply(links, function(l) try(getJobDescription_f(getURLContent(l)))), 
                        names = links)
      
      jobs = c(jobs, posts)
      
      # determines the next page of results
      curPage = getNextPage_f(listings_page)
      
      # if we cannot parse curPage, exit while loop
      t <- try(read_html(curPage))
      if(inherits(t,"try-error")){
        break
      } 
      pageNum = pageNum + 1
      #print(pageNum)
    }
    
    invisible(jobs[!sapply(jobs, inherits, "try-error")])
    return(jobs)
}
```

## Scraping CareerBuilder
##{.smaller}
We could define a site-specific function using searchJobs() for CareerBuilder. 

```{r,message=FALSE}
# Since the query string is not a value of a parameter on the right of the ? in the URL, we could not use getForm() to determine the first page of our search results
cb.searchURL <- function(query){
  each_word <- strsplit(query," ")[[1]]
  
  paste("https://www.careerbuilder.com/jobs-", 
        gsub(" ","-",query),
        "?keywords=",
        paste(each_word,collapse = "+"),sep = "")
}


# Get links to the job postings 
cb.getJobLinks <- function(page, base = baseURL){
  links = page %>% html_nodes(".show-for-medium-up a") %>%
    html_attr('href')
    
  getRelativeURL(links, base)
} 
  
# Get the link to next page
cb.getNextPage <- function(page){
  nxt = page %>%
    html_nodes('#next-button') %>%
    html_attr('href')
  
  if(length(nxt) == 0)
    return(character())
  return(nxt)
}

# get job descriptions
cb.getJobDescriptions <- function(job_url){
  job_post = read_html(job_url)
  descrip = job_post %>%
    html_node(".description") %>%
    html_text()
  
  return(descrip)
}

# Put it all together
searchCareerBuilders <-function(query, ..., baseURL = 
             'https://www.careerbuilder.com/jobs?keywords=&location=',max = NA)
{
  # make baseURL a global variable, so that we can use it in other function
  baseURL <<- baseURL  
  
  # Determine the first page of search results
  txt = cb.searchURL(query)
    
  searchJobs(firstPage = txt, getNextPage_f = cb.getNextPage, 
               getJobDescriptionLinks_f = cb.getJobLinks, 
               getJobDescription_f = cb.getJobDescriptions, max = max)
}

allposts = searchCareerBuilders(query = "Data Scientist", max = 100)
length(allposts)
```

## Scraping SimplyHired.com
##{.smaller}
We can follow the same recipe to scrapy SimplyHired.com
```{r,message=FALSE}
# Get links to the job postings 
sh.getJobLinks <- function(page, base = baseURL){
  links = page %>% 
    html_nodes(".js-job-link") %>%
    html_attr('href')
  
  getRelativeURL(links, base)
} 

# Get the link to next page
sh.getNextPage <- function(page, base = baseURL){
  nxt = page %>%
    html_nodes('.next-pagination a') %>%
    html_attr('href')
  
  nxt = getRelativeURL(nxt, base)
  
  if(length(nxt) == 0)
    return(character())
  return(nxt)
}

# get job descriptions
sh.getJobDescriptions <- function(job_url){
  Sys.sleep(8)
  job_post = read_html(job_url)
  descrip = job_post %>%
    html_nodes('.viewjob-description') %>%
    html_text()
  
  return(descrip)
}


# Put it all together
searchSH <-function(query, ..., baseURL = 
             'https://www.simplyhired.com/search',max = NA)
{ 
  # Determine the first page of search results
  baseURL <<- baseURL
  txt = getForm(baseURL, q = query)
  
  searchJobs(firstPage = txt, getNextPage_f = sh.getNextPage, 
               getJobDescriptionLinks_f = sh.getJobLinks, 
               getJobDescription_f = sh.getJobDescriptions, max = max)
}

alljobs = searchSH(query = "data scientist", max = 1)
alljobs[[1]]
```

## Some advice
##{.smaller}

Download the html/json files you aim to scrape (e.g. locally or cached using HTTrack). Ideally this is should be done once, otherwise you are risking getting your IP banned or blacklisted.

```{r, message = FALSE}
# Optional commands to download htmls locally
library(purrr)
possibly_download_file = possibly(download.file, otherwise = NA, quiet = TRUE)
safely_download_file = safely(download.file, otherwise = NA, quiet = TRUE)
```